{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQDE07O8/8pYqfI3AMjJjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cylia-ABBAD/Factorization_Machines_Report/blob/main/Report%20FMs\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6suWCVbg_Ra"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_TLUFwChAWK"
      },
      "source": [
        "# Factorization Machines \r\n",
        "\r\n",
        "\r\n",
        "## 1.Introduction\r\n",
        "\r\n",
        "Various models can be used for machine learning depending on the task to be performed. Supervised learning is one of the tasks, where a function is learned that maps an input to an output based on example input-output pairs. Support Vector Machines (SVMs) are a set of supervised learning techniques designed to solve discrimination and regression problems. They are one of the most popular predictors in Machine Learning. However in some cases, SVMs play no important role because they can't learn reliable parameters (hyperplanes) in non-linear kernel spaces under very sparse data (data where only a few variables for each input vector of predictor variables are non-zero). Thus, the best models are either direct applications of standard matrix / tensor factorization models or specialized models using factorized parameters.  However, these models also have the disadvantage that they are not applicable to standard prediction data.\r\n",
        "\r\n",
        "In this report, a new supervised machine learning model, called Factorization Machines (FMs) will be introduced. It's a new class of models that combines the advantages of SVMs and factorization models. FMs are especially popular in predicting the click-through rate (CTR) or for recommender systems. FMs are an impactful model and have shown excellent prediction capabilities. They have many advantages: \r\n",
        "\r\n",
        "- FMs  are a general predictor that can work with any real valued feature vector. \r\n",
        "- FMs  model all interactions between variables using factorized parameters.  \r\n",
        "- FMs  are capable of estimating interactions in problems with a high degree of sparsity where SVMs fail.\r\n",
        "- FMs  have linear complexity, can be optimized in the primal and don't rely on support vectors like SVMs.\r\n",
        "\r\n",
        "This report will introduce FMs, their propreties, comparison to other models and finally some  important conclusions. \r\n",
        "\r\n",
        "## 2.Factorization Machine\r\n",
        "\r\n",
        "### 2.1. Factorization Machine Model\r\n",
        "\r\n",
        "#### a) Model Equation: \r\n",
        "The most common prediction task is to estimate a function $ y :\\mathbb{R}^n → T \\quad$  from a real valued feature vector $x \\in \\mathbb{R}^n$ to a target domain $T$.  Let $m(x)$ be the number of non-zero elements in the feature vector $x$ and $\\bar{m}_D$  be the average number of non-zero elements $m(x)$ of all vectors $x \\in D$,  where  D={$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),... $}  is the training dataset. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "For a factorization machine of degree d = 2 (also called a 2-way FM ), the model equation is defined as:\r\n",
        "\r\n",
        "\r\n",
        "$ \\hat{y}(x)=w_0 + \\sum_{i=1}^{n} w_i x_i + \\sum_{i=1}^{n}  \\sum_{j=i+1}^{n} <v_i,v_j>x_i x_j  \\qquad $             (1)\r\n",
        "\r\n",
        "where the model parameters that have to be estimated are:\r\n",
        "\r\n",
        "$ w_0 \\in \\mathbb{R} , \\bf{w} \\in \\mathbb{R}^n , \\bf{V} \\in \\mathbb{R}^{nxk}  \\qquad $  (2) $\\qquad$  With  $ \\quad \\bf{<v_i,v_j>}$=  $ \\sum_{f=1}^{k} v_{i,f} v_{j,f} \\qquad $ (3)\r\n",
        "\r\n",
        "\r\n",
        "A row $\\bf{v_i}$ within $\\bf{V}$ describes the i-th variable with $k$ factors, $k \\in  \\mathbb{N}_{0}^{+} $  is a hyperparameter that defines the dimensionality\r\n",
        "of the factorization.\r\n",
        "\r\n",
        "A 2-way FM captures all single and pairwise interactions between variables:\r\n",
        "\r\n",
        "- $w_{0}$ is the global bias.\r\n",
        "- $w_i$ models the strength of the i-th variable.\r\n",
        "- $\\hat{w}_{i,j} = <v_i,v_j> $  models the interaction between the ith and j-th variable.    \r\n",
        "\r\n",
        "The FM models the interaction by factorizing it instead of using an own model parameter  $w_{i,j} \\in \\mathbb{R}$  for each interaction.\r\n",
        "\r\n",
        "\r\n",
        "#### b) Expressiveness :\r\n",
        "\r\n",
        "We know that if a matrix $W$ is positive definite ,  there exists a matrix $V$ such that $W =V · V^t$  provided that $k$ is sufficiently large, under this condition, a FM can express any interaction matrix $W$. However in sparse settings, when there is not enough data to estimate complex interactions $W$, one should chose a small $k$. In this case, restricting $k$ (and thus the expressiveness of the FM) improves interaction matrices under sparsity.\r\n",
        "\r\n",
        "#### c) Parameter Estimation Under Sparsity: \r\n",
        "\r\n",
        "In sparse settings, there is not enough data to estimate directly and independently the interactions between variables , but even in this case,  FMs can estimate these interactions  well by breaking the independence of the interaction parameters by factorizing them. \r\n",
        "\r\n",
        "### d) Computation\r\n",
        "\r\n",
        "The complexity of straight forward computation of eq.(1) is in $O(k n^2 )$ \r\n",
        "because all pairwise interactions have to be computed. By reformulating pairwise interactions, it drops to linear runtime as follows :\r\n",
        "\r\n",
        "\r\n",
        "![](http://berwynzhang.com/assets/image/2017-01-22/1484302786060.png)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "This equation has only linear complexity, its computation is in $O(k n)$.\r\n",
        "In the case of sparsity, most of the elements in $x$ are $0$, so the sums have only to be computed over the non-zero elements. Thus, the computation of the factorization machine is in $O(k \\bar{m}_{D})$ .\r\n",
        "\r\n",
        "### 2.2 Factorization Machines as Predictors\r\n",
        "FM can be applied to a variety of prediction tasks :\r\n",
        "\r\n",
        "- Regression\r\n",
        "- Binary classification\r\n",
        "- Ranking\r\n",
        "\r\n",
        "In order to prevent overfitting, regularization terms like $L2$ are usually added to optimization objective.\r\n",
        "\r\n",
        "\r\n",
        "### 2.3 Learning Factorization Machines\r\n",
        "\r\n",
        "As said before, FMs have a model equation with a linear complexity. Thus, the model parameters $(w_0, w, V)$ can be learned efficiently by gradient descent methods for a variety of losses, among them are square, logit or hinge loss.\r\n",
        "The gradient of the FM model is calculated as follows:\r\n",
        "\r\n",
        "![](http://berwynzhang.com/assets/image/2017-01-22/1484553331640.png)\r\n",
        "\r\n",
        "$ \\sum_{j=1}^{n} v_{j,f} x_j$  can be precomputed since it's  independent of $i$ .\r\n",
        "\r\n",
        "### 2.4 d-way Factorization Machine\r\n",
        "\r\n",
        "We can easily generalize the 2-way FM to a d-way FM :\r\n",
        "\r\n",
        "![](http://berwynzhang.com/assets/image/2017-01-22/1484558701251.png)\r\n",
        "\r\n",
        "The straight-forward complexity for computing eq. (5) is\r\n",
        "$O(k_d n^d)$, but as done before for eq.(1), one can show that it can be computed in linear time.\r\n",
        "\r\n",
        "\r\n",
        "### 2.5  FMS vs. SVMS\r\n",
        "\r\n",
        "SVMs are one of the most popular models in machine learning, which use the kernel trick to find an optimal boundary between possible outputs. The optimal boundary, called $\\bf{hyperplane}$, is influenced by the support vectors which are data points close to the hyperplane.\r\n",
        "When using a polynomial kernel in SVMs, both FMs and SVMs model nested interactions between variables. But using SVMs, the weights given to those interactions between variable $i$ and $j$ $(w_{i,j})$ is independent of the weight given to the interaction between variable $i$ and $k$ $(w_{i,k})$. While using FMs, those weights are factorized, ie $w_{i,j}$ and $w_{i,k}$ depend on each other as they overlap and share parameters due to the factorization. Another advantage is that making a prediction using FMs with its model equation obtained using training data, is independent of this training data. While using SVMs the prediction depends partly on the training data, because some data points in this training data have some influence on the model equation.The two mentioned advantages of FMs over SVMs are particularly present when using very sparse data, as SVMs cannot learn reliable parameters in this case. Besides, using (nonlinear) SVMs, the data usually needs to be transformed to the dual form unlike FMs which can learn the model parameters without transforming the data to the dual form.  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "### 2.6 Other Types of Factorization Models vs. Factorization Machines\r\n",
        "\r\n",
        "Several factorization models are often used for prediction tasks (particularly click-through rate (CTR) prediction and Recommender Systems). For instance, the use of a factorization model, namely Matrix Factorization (MF), in the Netflix Challenge. The task in this challenge was to predict user ratings for movies offered by Netflix based on previous ratings. \r\n",
        "\r\n",
        "A distinction can be made between standard factorization models (like PARAFAC or MF), which factorize a relation between categorical variables and specialized factorization models (like SVD++, PITF or FPMC), which are used for specific data and tasks. The benefit of FMs over standard factorization models is that they can work with any real-valued feature vector. This in contrast to the standard factorization models, which require feature vectors that are divided in $n$ groups (when having $n$ categorical variables) and in each group exactly one value has to be 1 and the rest 0. Therefore, FMs are a general predictor. Specialized factorization models are designed for a single task. This results in many different published articles, and it's also showed that FMs can mimic many of the most successful factorization models just by feature extraction. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "## 3. Conclusion\r\n",
        "\r\n",
        "In this report, we introduced Factorization Machine, how it works and its different properties. Below are some of the conclusions that we have been able to draw about the different models we have studied: \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "- FMs model all possible interactions between values in the feature vector x using factorized interactions instead of full parametrized ones. This has 2 main advantages:\r\n",
        "\r\n",
        "   - The interactions between values can be estimated even under high sparsity. Especially, it is possible to generalize to unobserved interactions.\r\n",
        "\r\n",
        "   - The number of parameters as well as the time for prediction and learning is linear.\r\n",
        "   \r\n",
        "- The dense parametrization of SVMs requires direct observations for the interactions which is often not given in sparse settings. \r\n",
        "\r\n",
        "- Parameters of FMs can be estimated well even under sparsity.\r\n",
        "\r\n",
        "- FMs can be directly learned in the primal. Non-linear SVMs are usually learned in the dual.\r\n",
        "\r\n",
        "- The model equation of FMs is independent of the training data. Prediction with SVMs depends on parts of the training data.\r\n",
        "\r\n",
        "- Standard factorization models like PARAFAC or MF are not general prediction models like factorization machines. Instead they require that the feature vector is partitioned in m parts and that in each part exactly one element is 1 and the rest 0.\r\n",
        "\r\n",
        "- FM can mimic other specialized models(such as SVD++, PITF, FPMC) well.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "$\\bf{Factorization \\quad  Machine \\quad is \\quad  a \\quad  must-have \\quad algorithm \\quad in \\quad any \\quad data  \\quad scientist’s \\quad  pocket !}$\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#### References:\r\n",
        "- [link](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)\r\n",
        "- [link](https://pdf.sciencedirectassets.com/271506/1-s2.0-S0957417418X00153/1-s2.0-S0957417418303270/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFMaCXVzLWVhc3QtMSJGMEQCIBrZOTqvJm3sp71p5iLqEAOtO%2BcJqxtyt7Qrl68kNLqyAiA0%2BB%2BD7Rs%2BNY5wpyZ9EAE6P63YmFb3%2Fr%2B9TlhhZYRB6iq0AwgcEAMaDDA1OTAwMzU0Njg2NSIM4y8osr9%2FrRI91Ah%2FKpEDuD990Wj666Z8e1q%2Bn8lTSrGIea%2BZZqd4945j5wwA0roL0uRsM0cq84UPpeZUCCD%2FPKfKmFKZUayciHWd7iznhNqgSplMANt7Siiov1E3aUUahNWxsDNg4YCV%2BXs7ysRT02GclV4sTbncHifyu%2F2scdKn1YvlIeUqVPBxxxFhTcmvPWl%2FKEBg412A93pPP3RqZINlIsxmcW0S%2BcZEJ81K50mGod85rAG6GXU14oSxvpfcMiZN3K4JuCr77BtS03rrtzZLVC%2FFKcSI9YDlN%2Fr%2BPLWQZD7TnXKDxxNe%2F3ilYefZmX%2B5KLUUANwm%2FqPDRx1fLRR9mNzapj4DiIXW675CsL2g55oDsyUnPdLDIFXeb6ym2NVGxSgnGRKkNzmJ5nszIHuwt0VJzYoCkbAl16DijG5bh6il8gm7TQ5HPq9uJER3dn257AG%2BPSCVlVAJ0cczwRhGDnF0uXr8URIcmSW18B0ANl88MgLV57PXfSxJiIV2B4XiOvyKym1nafkEtuoy4T2pnfw79BehUno8w5ZtjFIwsOPS%2FwU67AH04zMXWEp5rQiHf%2F%2B9ON6OsFxRkBANugM203rbkEkFQ54P2FbMaSgRQpZ021qzBCgvilViROJI8omtCXEtJQB8f8AKzsSqTQ1%2BpjX44LSW%2BxewwEsMehNsTppcVACtKTDbbk04F%2BNkEew2OAbToVnRpAqUKD%2BtynTgC1ovHUHVyFkOnCnCCnqU3ada0sqKQO1OsXNfg5%2Fp7WbrCgq9Kr%2BWs%2B9JpB%2Fg9tU3gNkd8A1zNYpqThJq5mR1Yu%2FCW8HvlTVhWuYbuxSaAV4ehavwJzkLXhfEsYozkCFoW8lB8q8AaavxTb%2FFQyaYXE2Cdw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210105T201518Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYX75IIN7N%2F20210105%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=1cda892761209b582e9ce6bf1654d44ccd9be5a061ecaac840d5288a8f49fe63&hash=81c470e695471b87968772f565606129cc3feb59919cc81a8bb7fdfc82a18310&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0957417418303270&tid=spdf-c05e3351-a59f-45e8-b2c9-d2fd576720e2&sid=a3d04e9d379c544f1e0acbe9c10943046990gxrqb&type=client)"
      ]
    }
  ]
}